{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CSCCNY/final-project-recomposeclassics/blob/main/Report_RecomposeClassics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGogWFK7MpaQ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **Compose Chopin**\n",
    "#### Learn Chopin's style of composition through CNN, AutoEncoder, GANs and Pix2Pix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef9ScunNMpI4"
   },
   "source": [
    "Bethold Owusu, Hannah Do, Ethan Cobb\n",
    "\n",
    "**CSCI1910**  /  Spring 2021\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ztB3WbIMo0Y"
   },
   "source": [
    "### **Abstract**\n",
    "\n",
    "Our project aims to learn Chopin's style of composition through various DNN models such as CNN, LSTM, AutoEncoder, and GANs. Training a machine how to compose has been in demand for awhile, and many state of the art models such as Amper Music, AIVA, Jukedeck, and others have been developed with AI composers. However, no commercial AI system has been good enough by itself, and many composers use it as a mere reference. Therefore, we have taken a different approach in converting the wav files into spectrograms, and predicting the pieces as an image analysis process. In other words, our project focuses on predicting the next sequence of a song given a previous sequence of a song using spectrogram pixel values. We have developed different DNN models to produce machine-composed segments that resemble target music pieces with high similarity scores. Converting the midi and wav files into spectrograms unavoidably added some noises to the song, however complex CNN model with transpose layers and AutoEncoder model resulted in higher prediction accuracy than the Base models with low mse and mae scores when compared with the actual sequence. In addition, VAE, Transformer, DCGAN, and PIX2PIX models were tested and the last model, PIX2PIX resulted in highly accurate conversion in recreating the sequence of the songs. Having such a result, we hope to expand this project not only in Chopin style, but to different composing styles in order to allow composing songs as easy as clicking a button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9MvKNBXS8xX"
   },
   "source": [
    "### **Introduction**\n",
    "\n",
    "Our research question was how to train the song sequences from a dataset to predict a new sequence, enabling a trained machine to compose songs by itself. To begin with, we selected a classical music midi file dataset from Kaggle. Compared to other datasets that contain different instrumentals and information about the file itself, our goal was to train the model with a clean midi file dataset that only contains piano tunes in refined forms. As we wanted to predict a composition style of a single composer, the composer with the most songs in the dataset - Chopin was selected.\n",
    "In researching different types of ways to pre-process the midi files, we found many state of the art models using spectrogram conversion for data analysis, using image analysis method to extract a new spectrogram to predict following sequences. As a spectrogram shows the power of various frequencies in the song over time in an image, we decided to move on with converting the midi files to spectrograms. The specific process included processing the midi into wav files first, then converting the wav files into spectrograms through Librosa library. And with the spectrogram arrays stacked together to form 3D numpy arrays, different DNN models such as CNN, LSTM, Transformer, AutoEncoders, DCGAN, Pix2Pix were trained to predict the final sequence array. The procedure and detailed results are explained in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFdPciEPS_qu"
   },
   "source": [
    "### **Background**\n",
    "\n",
    "\n",
    "There has been significant progress in the generation of music using deep neural network architectures. GAN’s have been shown to be successful at composing in the style of particular artists or genres, and LSTM-RNN networks have also shown to be effective at generating melodies, albeit their extremely long training times. While the majority of these network architectures have focused on producing music in the style of whatever is inputted into the model, not as much work has been done for the specific task of completing pieces fed as input to the model. \n",
    "This kind of problem - feeding half of a particular piece to a DNN and having it finish the rest of the piece - lends itself quite nicely to classical music applications. There have been many pieces composed throughout the Classical, Baroque and Romantic periods that were left unfinished - Schubert’s 9th symphony is perhaps one of the most famous of these instances. This kind of DNN application could prove to be incredibly interesting to not only AI Audio Engineers but to those on the more qualitative end - musicologists, music historians, ethnomusicologists, etc. \n",
    "We could have chosen any number of pieces to train on, but we chose one composer - Chopin - to simplify the model and our understanding of our outputs. By only considering one composer, it is easier to develop an “ear” for what should sound like similar output. \n",
    "As stated above, although the majority of literature in AI music has focused on generative models to compose music in a particular style or genre, none has focused in particular on the task of completing a specific piece of music. In terms of the overall model and training architectures, there is not necessarily any novelty compared to the literature, however the problem formulation itself is unique. This unique approach lends itself to calculating various loss and accuracy scores since we know exactly what the results should be (for the other half). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSk8Cb1US_8F"
   },
   "source": [
    "### **Data**\n",
    "\n",
    "1. Dataset selection and File Format Conversion\n",
    "\n",
    "Upon researching different types of datasets, we have decided to select midi files as our source as the format conserves piano tunes in a finer form compared to other instrumentals or audio files, which may add different types of noise into our data. And we decided to choose a dataset from a reliable source - Kaggle, and found a dataset called ‘Classical Music MIDI’ with a total 7 MB of data with over thousands of downloads history ( https://www.kaggle.com/soumikrakshit/classical-music-midi ). Considering we had to train the models with songs from one composer, we selected a composer with the most number of songs, which was Chopin. We initially started with the 48 midi files of Chopin’s different piano pieces, however converted them to wav files in order to pre-process them into spectogram arrays and segmented the wav files into 15 seconds time intervals with 5 seconds of overlap, resulting in over 2700 wav files.\n",
    "\n",
    "2. Dividing the data into Train-Test, Input-Output\n",
    "\n",
    "We splitted the total dataset into train, test and input, output data before the conversion of wav files into arrays since splitting them after converting them to arrays resulted in loss in sound quality. After we have splitted the data into four parts, with the train and test ratio as 5:1, we converted the wav files into spectogram arrays using Fluidsynth library. Since the spectogram in general shows the power of various frequencies in the song over time in an image, it allowed us to train the dataset with machine learning models such as CNN, Autoencoder, DCGAN, and PIX2PIX.\n",
    "\n",
    "3. Cleaning and Pre-processing the Data\n",
    "\n",
    "Before converting the wav files into spectograms, any files that were cut off under 15 seconds - the leftover segments were removed to reduce any outliers. And using Librosa, the waveplots and spectograms were visualized to make sure the spectograms were converted adequately. The segmenting process used AudioSegment function from Pydub library used to manipulate audio with high level interface in python, preserving the audio quality while dividing them into smaller pieces. In addition, with the Librosa library, the wav files were converted into a spectogram with the method of squaring the magnitude of the short term Fourier transform (STFT), then compressed to a mel-spectograms. This made sure all the data input and outputs were in range of appropriate spectogram scale. Each spectogram arrays were stacked together to  In addition, Bethold converted the spectograms back to wav files to ensure the rhythm and tempo is preserved. Below shows the original audio and that of the spectrogram.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"results/spec_2_audio.JPG\" width=\"400\">\n",
    "<h5 align=\"center\">Figure 1. Spectograms converted back to Wav Files\n",
    "</h5> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Reshaping\n",
    "\n",
    "Based on different models, the spectrogram arrays had to be reshaped in order to fit the models. For the baseline models, such as Random Forest and Linear Regression, the input and output shapes were flattened to 1D arrays with reshape(-1,1) function in order to be trained to evaluate mae and mse scores. And as the conversion between melspectogram and wav file required a 2D array, the dimension of the stacked 3D model was expanded to 4D array for regression prediction with np.expand_dims(data, -1). The final shape of train and test data for CNN models were (1275, 128, 646, 1) and (158, 128, 646, 1).\n",
    "\n",
    "For Autoencoder, dimensions for the input were (1575, 256, 64, 1) and the same for the decoder, while the dimension of the latent space was 2. And for Transformer, GANs, and Pix2Pix models, an additional procedure of converting the grayscale spectogram to color spectogram was used to conserve more data, resulting in train and test data shapes of (2, 256, 300, 128, 1) and (2, 20, 300, 128, 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThlBUkGlMmlh"
   },
   "source": [
    "### **Methods**\n",
    "\n",
    "\n",
    "1. **Baseline Models**: Linear Regression and Random Forest\n",
    "\n",
    "For the baseline models, the input and output shapes were flattened to 1D arrays with reshape(-1,1) function in order to be trained to evaluate mae and mse scores. Simple logistic regression was performed without any regularization and following is the result, the mae score of 9.34 and mse score of 167.16.\n",
    "\n",
    "<img src=\"results/lg_result.PNG\" width=\"400\"> \n",
    "\n",
    "Meanwhile the Random Forest model was set up with the optimal architecture after cross-validation as the following :\n",
    "\n",
    "<img src=\"results/rf_model.PNG\" width=\"700\"> \n",
    "\n",
    "After training with the normalized train dataset and its target, the model was evaluated on the test dataset and as a result, an R square of 0.4 was obtained. Another metric of R square was utilized here as the baseline model is performing a regression prediction based on the train and its target dataset.\n",
    "\n",
    "\n",
    "\n",
    "2. **CNN** : Hannah\n",
    "\n",
    "From the prepared spectogram arrays of 1275 train data and 158 test data for each input and output set, three different types of CNN models were run to predict the output array. The first model was a basic CNN model with minimum parameters with max pooling, drop out, conv 2d, transpose, and leaky relu layers embedded one time each. Total and trainable number of parameters were 29 and the model was compiled with adam optimizer to predict the mse and mae scores. \n",
    "\n",
    "  <img src=\"results/cnn_model1.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model1_result.PNG\" width=\"400\"> \n",
    "\n",
    "\n",
    "Running 10 epochs of model one definitely showed steady decrease in loss value, however did not converge fast enough and could be run with more epochs for better fitting.\n",
    "\n",
    "\n",
    "The second model was developed with more deeper convolution layers, with narrower layers with only few nodes in each layer, a total number of parameters of 800.\n",
    "<img src=\"results/cnn_model2.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model2_result.PNG\" width=\"400\"> \n",
    "The training and validation loss overlapped around epoch eight for model two. The training loss may continue to steadily decrease, but the model training seems to be stopped early for generalization trade-off. The loss scores were much lower than the first model.\n",
    "\n",
    "\n",
    "The third model was developed with more deep convolution layers, along with additional nodes on each layers, the total number of parameters of 38,611. \n",
    "<img src=\"results/cnn_model3.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model3_result.PNG\" width=\"400\"> \n",
    "The validation loss started to increase around epoch seven for model three. And following is a graph summarizing performance of all three models.\n",
    "\n",
    "<img src=\"results/cnn_models_result.PNG\" width=\"400\"> \n",
    "Here, we can see that model one is out of bounds since the convergence is too slow and model three is shown to perform the best. Observing this result, model three was chosen to train the dataset and the detailed evaluation is continued in the next section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **AutoEncoder** : Ethan\n",
    "\n",
    "Borrowing from Valerio Velardo, a notable figure in the AI Audio community, I implemented a preprocessing pipeline that involves the following steps: \n",
    "- Load the midi files\n",
    "- Convert the midi files to wav files\n",
    "- Left or right pad the signal (if necessary)\n",
    "- Extract log spectrogram of signal\n",
    "- Save normalized spectrogram as .npy file\n",
    "\n",
    "\n",
    "At the end of this preprocessing pipeline, you end up with a directory that contains all of the normalized log spectrograms of the samples, as well as a pickle file file that contains the min and max values of the spectrograms, to be used for normalization (before being fed into the model) and eventually for denormalization (to go back to normal audio after conversion from spectrogram to audio). Log spectrograms were extracted using librosa’s short-time Fourier transform function with log scaling. \n",
    "\n",
    "As for the Variational Autoencoder, I used a 5 convolutional layer architecture with mirrored encoder and decoder components and a two-dimensional latent space. I trained it for 100 epochs across all 1575 samples (extracted from original full MIDI files of Chopin pieces). \n",
    "\n",
    "\n",
    "\n",
    "4. **Transformer, GANs, Pix2Pix** : Bethold\n",
    "\n",
    "I converted each of the spectrograms to a numpy array of (512,128), then I splitted into input data and target data of (256, 128) array respectively. Thus given an input audio, the goal is to predict the rest of the audio song. So the 15 seconds audio files were split into 5 seconds input and 5 seconds target. Below shows sample spectrogram for input and target 5 seconds audio files.\n",
    "\n",
    "<img src=\"results/inp_target.JPG\" width=\"400\"> \n",
    "Figure 2: Above shows sample spectrograms of input and target of 5 seconds respectively\n",
    "\n",
    "Total input and target data were (1431, 256, 128) numpy arrays respectively. From here I normalized the data to be between 0 and 1 by subtracting each value from the max and dividing the entire values by max. In this work there was no need to explore univariate and multivariate feature selection or dimension reduction  because the wav files transformed into mel-spectrograms had the important features preserved. \n",
    "\n",
    "Next, I split the data into training and test data. The train consisted of 0.9 and test data consisted of 0.1 of the total input and target data. This gave us a training dataset of (1287, 256, 128) and test set of (144, 256, 128). \n",
    "\n",
    "Base models I utilized are Random Forest and tenor Regressor. I implemented 4 different models in addition to the base models. These models include Auto Encoders, Variational AutoEncoders, Deep Convolutional Generative Adversarial Network (DCGAN) and PIX2PIX. I utilized these models because we were dealing with image spectrograms and predicting pixel values. \n",
    "\n",
    "Hyperparameters I utilized include the number of hidden layers, filters and activation functions in each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NG1mWiZMmSo"
   },
   "source": [
    "#### **Evaluation**\n",
    "\n",
    "\n",
    "1. **CNN** : Hannah\n",
    "\n",
    "\n",
    "Based on the previous results, model three was trained with the optimal epochs of seven and following is the final graph of training and validation loss for model three. Final mae score resulted in 8.9 for training and 8.4 for test data, much better than the baseline models.\n",
    "<img src=\"results/cnn_model3b_result.PNG\" width=\"300\"> \n",
    "<img src=\"results/cnn_model3c_result.PNG\" width=\"700\"> \n",
    "\n",
    "\n",
    "\n",
    "2. **AutoEncoder** : Ethan\n",
    "\n",
    "The final architecture for the variational autoencoder was as follows: \n",
    "\n",
    "Each layer in the encoder consists of a convolutional layer followed by a Relu activation and Batch normalization. Finally, they are flattened and mirrored in the decoder. Three measures of loss were calculated and an example of training with only 3 samples is shown below: \n",
    "The overall loss is provided by the reconstruction error multiplied by a weight plus the KL Divergence loss, the difference between a normal distribution from the standard normal distribution. All three measures of loss decreased throughout the range of epochs and showed no signs of overfitting. \n",
    "Due to the size of the dataset, hyperparameters had to be chosen to end up with a number of temporal frames equal to a power of 2 - this forced the duration to be fed into the stft librosa function to be .74 seconds. As a result, the predictions of the VAE were limited to very short snippets. The predictions themselves were made by first reshaping the log spectrogram (to omit the 3rd dimension reserved for the number of color channels), applying denormalization (using the saved max/min values) and finally applying the Griffin-Lim algorithm to convert the spectrogram to audio (what’s under the hood in the librosa implementation). \n",
    "\n",
    "\n",
    "\n",
    "3. **Transformer, GANs, Pix2Pix** : Bethold\n",
    "\n",
    "\n",
    "-  Performance of First Auto Encoder Model\n",
    "\n",
    "This model had 4 hidden layers with filters of 16, 32, 32, 16 filters respectively. The relu activation was utilized here. Below shows the model summary\n",
    " \n",
    "\n",
    "<img src=\"results/autoencoder.JPG\" width=\"400\">  \n",
    "First 4. Above show first autoencoder model summary\n",
    "\n",
    "I utilized the loss function of mean square error (mse) and Adam optimizer as optimization function with a learning rate of 0.001. After training with the normalized train dataset and its target, I obtained the lowest loss to be 130 after 50 epochs. \n",
    "\n",
    "<img src=\"results/autoencoder_loss_mse.JPG\" width=\"400\"> \n",
    "Figure 5. Above show training and validation loss of first model (autoencoder)\n",
    "\n",
    "I evaluated the test dataset and I obtained figure 6 below.\n",
    "\n",
    "<img src=\"results/sequence.JPG\" width=\"400\"> \n",
    "Figure 6. Above shows evaluation of the first model autoencoder on sample test data. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Performance of Second Auto Encoder Model\n",
    "\n",
    "This model had 5 hidden layers with filters of 32, 64, 128, 128, 256filters respectively. The relu activation was utilized here. Below shows the model summary\n",
    " \n",
    "\n",
    "<img src=\"results/autoencoder2.JPG\" width=\"400\"> \n",
    "First 7. Above show second autoencoder model summary\n",
    "\n",
    "I again utilized the loss function of mean square error (mse) and Adam optimizer as optimization function with a learning rate of 0.001. After training with the normalized train dataset and its target, I obtained the lowest loss to be 80 after 50 epochs. \n",
    "\n",
    "<img src=\"results/loss_func.JPG\" width=\"400\"> \n",
    "Figure 8. Above show training and validation loss of Second model with 5 hidden layer (autoencoder)\n",
    "\n",
    "I evaluated the test dataset and I obtained figure 9 below.\n",
    "\n",
    "<img src=\"results/auto_enc.JPG\" width=\"400\"> \n",
    "Figure 9. Above shows evaluation of the Second model autoencoder (5 hidden layers) on sample test data. \n",
    "\n",
    "\n",
    "\n",
    "- Performance of DCGAN (Third Model)\n",
    "\n",
    "The generative model had 6 hidden layers with filters of 256, 128, 128, 64, 32 filters respectively. The activation utilized here was leaky relu. The discriminator had 2 hidden layers of 64 and 128 filters respectively.\n",
    "\n",
    "Both the generator and discriminator had binary cross entropy as loss function. Adam optimizer was utilized as an optimization function with a learning rate of 0.0001. I trained for about 20 epochs.  \n",
    "\n",
    "Images generated from DCGAN along with a sample real image is shown below. \n",
    "\n",
    "<img src=\"results/dcgan_images.JPG\" width=\"400\"> \n",
    "Figure 10. Above shows sample test data (lower left) and images generated from DCGAN \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Performance of PIX2PIX (Fourth Model)\n",
    "\n",
    "The generative model had both upsample and downsample concatenated together with 4 hidden layers which had filters of 32, 64, 128 and 128 respectively. The downsample had an activation of leaky Relu while upsample had an activation relu. The discriminator had downsample of 3 hidden layers of 64, 128 and 256 filters respectively.\n",
    "\n",
    "Both the generator and discriminator had binary cross entropy as loss function. Adam optimizer was utilized as an optimization function with a learning rate of 0.0002 and beta of 0.5. I trained for about 30 epochs.  \n",
    "\n",
    "<img src=\"results/disc_loss_1.JPG\" width=\"400\"> \n",
    "<img src=\"results/gen_loss_1.JPG\" width=\"400\"> \n",
    "Figure 11. Above shows the loss for the discriminator and generator \n",
    "\n",
    "I evaluated the PIX2PIX model on the test dataset and I obtained figures below.\n",
    "\n",
    "\n",
    "<img src=\"results/spec2.JPG\" width=\"400\">  \n",
    "<img src=\"results/spec10.JPG\" width=\"400\"> \n",
    "Figure 12. Above shows input , target and predicted results from PIX2PIX model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azibEYIUMm6Q"
   },
   "source": [
    "### **Conclusion**\n",
    "\n",
    "After obtaining spectrogram results from all models, I converted back to wav file and evaluated audio results. Below are audio results from all models\n",
    "\n",
    "Audio result from model 1 (Auto Encoder 1)\t\tAudio result from model 2 (Auto Encoder 2)\n",
    "\n",
    "<img src=\"results/audio results.JPG\" width=\"400\">\n",
    "<img src=\"results/audio_autoencoder2.JPG\" width=\"400\">\n",
    "Figure 13. Above shows audio results from model 1 (left) Auto Encoder 1 and audio results from model 2 (right) Auto Encoder 2\n",
    "\n",
    "\n",
    "Audio result from model 3 (DCGAN)\t\tAudio result from model 4 (PIX2PIX)\n",
    "\n",
    "<img src=\"results/audio_dcgan.JPG\" width=\"400\">\n",
    "<img src=\"results/audio_pix2pix.JPG\" width=\"400\">\n",
    "Figure 14. Above shows audio results from model 3 (left)-DCGAN and audio results from model 4 (right)-PIX2PIX.  \n",
    "\n",
    "\n",
    "As shown in figure 13 and 14, the 2 Auto Encoders and DCGAN did not produce any meaningful audio sound. Thus, the predicted audio wav were not close to the target wave audio. The predicted wav from model 1 (Auto Encoder 1), model 2 ( Auto Encoder 2) and model 3 (DCGAN) were hashed and noisy audio. \n",
    "\n",
    "On the other hand, the PIX2PIX model generated much better spectrograms and wav audio as shown in figure 14. The predicted wav audio sound very close to the target audio. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suxnAKw8TAPX"
   },
   "source": [
    "### **Attribution**\n",
    "\n",
    "Using the number and size of github commits by author (bar graph), and the git hub visualizations of when the commits occurred. Using these measures each person should self-report how many code-hours of their work are visible in the repo with 2-3 sentences listing their contribution. Do not report any code hours that cannot be traced to commits. If you spend hours on a 2-line change of code or side-reading you did, you cannot report. If you do searches or research for the project that does not result in code, you must create notes in a markdown file (eg. in the project wiki) and the notes should be commensurate with the amount of work reported. Notes cannot be simply copy-pasted from elsewhere (obviously).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sqxuihTUP9f"
   },
   "source": [
    "### **Bibliography**\n",
    "\n",
    "Briot, Jean-Pierre, et al. “Deep Learning Techniques for Music Generation -- A Survey.” ArXiv:1709.01620 [Cs], Aug. 2019. arXiv.org, http://arxiv.org/abs/1709.01620.\n",
    "\n",
    "Rivero, Daniel, Iván Ramírez-Morales, Enrique Fernandez-Blanco, Norberto Ezquerra, and Alejandro Pazos. \"Classical Music Prediction and Composition by Means of Variational Autoencoders.\" Applied Sciences 10, no. 9 (2020): 3053. doi:10.3390/app10093053.\n",
    "\n",
    "Szelogowski, Daniel. “Generative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers.” ArXiv:2101.00169 [Cs, Eess], Apr. 2021. arXiv.org, http://arxiv.org/abs/2101.00169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bvcG3f0UZzm"
   },
   "source": [
    "### **Appendix**\n",
    "\n",
    "If there are minor results and graphs that you think should be included, put them at the end. Do not include anything without an explanation. No random graphs just for padding!! However, let’s say you did a 50 state analysis of poverty and demographics, and your report focused on the 5 most interesting states, for completeness you could include all in an appendix. Be sure though to provide some (very short) discussion with each figure/code/result.\n",
    "\n",
    "Again, this is not an opportunity to explain what you learned on the project or in the course, or express regret on mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNDyjNe6Mlun"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcQeqoepU0/qpSSV8Z+HmX",
   "include_colab_link": true,
   "name": "Report_RecomposeClassics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CSCCNY/final-project-recomposeclassics/blob/main/Report_RecomposeClassics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGogWFK7MpaQ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **Compose Chopin**\n",
    "#### Learn Chopin's style of composition through CNN, LSTM, AutoEncoder, and GANs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef9ScunNMpI4"
   },
   "source": [
    "Bethold Owusu, Hannah Do, Ethan Cobb\n",
    "\n",
    "**CSCI1910**  /  Spring 2021\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ztB3WbIMo0Y"
   },
   "source": [
    "#### **Abstract**\n",
    "\n",
    "Our project aims to learn Chopin's style of composition through various DNN models such as CNN, LSTM, AutoEncoder, and GANs. Training a machine how to compose has been in demand for awhile, and many state of the art models such as Amper Music, AIVA, Jukedeck, and others have been developed with AI composers. However, no commercial AI system has been good enough by itself, and many composers use it as a mere reference. Therefore, we have taken a different approach in converting the wav files into spectrograms, and predicting the pieces as an image analysis process. In other words, our project focuses on predicting the next sequence of a song given a previous sequence of a song using spectrogram pixel values. We have developed different DNN models to produce machine-composed segments that resemble target music pieces with high similarity scores. Converting the midi and wav files into spectrograms unavoidably added some noises to the song, however complex CNN model with transpose layers and AutoEncoder model resulted in higher prediction accuracy than the Base models with low mse and mae scores when compared with the actual sequence. In addition, VAE, Transformer, DCGAN, and PIX2PIX models were tested and the last model, PIX2PIX resulted in highly accurate conversion in recreating the sequence of the songs. Having such a result, we hope to expand this project not only in Chopin style, but to different composing styles in order to allow composing songs as easy as clicking a button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9MvKNBXS8xX"
   },
   "source": [
    "#### **Introduction**\n",
    "\n",
    "Our research question was how to train the song sequences from a dataset to predict a new sequence, enabling a trained machine to compose songs by itself. To begin with, we selected a classical music midi file dataset from Kaggle. Compared to other datasets that contain different instrumentals and information about the file itself, our goal was to train the model with a clean midi file dataset that only contains piano tunes in refined forms. As we wanted to predict a composition style of a single composer, the composer with the most songs in the dataset - Chopin was selected.\n",
    "In researching different types of ways to pre-process the midi files, we found many state of the art models using spectrogram conversion for data analysis, using image analysis method to extract a new spectrogram to predict following sequences. As a spectrogram shows the power of various frequencies in the song over time in an image, we decided to move on with converting the midi files to spectrograms. The specific process included processing the midi into wav files first, then converting the wav files into spectrograms through Librosa library. And with the spectrogram arrays stacked together to form 3D numpy arrays, different DNN models such as CNN, LSTM, Transformer, AutoEncoders, DCGAN, Pix2Pix were trained to predict the final sequence array. The procedure and detailed results are explained in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFdPciEPS_qu"
   },
   "source": [
    "#### **Background**\n",
    "\n",
    "\n",
    "There has been significant progress in the generation of music using deep neural network architectures. GAN’s have been shown to be successful at composing in the style of particular artists or genres, and LSTM-RNN networks have also shown to be effective at generating melodies, albeit their extremely long training times. While the majority of these network architectures have focused on producing music in the style of whatever is inputted into the model, not as much work has been done for the specific task of completing pieces fed as input to the model. \n",
    "This kind of problem - feeding half of a particular piece to a DNN and having it finish the rest of the piece - lends itself quite nicely to classical music applications. There have been many pieces composed throughout the Classical, Baroque and Romantic periods that were left unfinished - Schubert’s 9th symphony is perhaps one of the most famous of these instances. This kind of DNN application could prove to be incredibly interesting to not only AI Audio Engineers but to those on the more qualitative end - musicologists, music historians, ethnomusicologists, etc. \n",
    "We could have chosen any number of pieces to train on, but we chose one composer - Chopin - to simplify the model and our understanding of our outputs. By only considering one composer, it is easier to develop an “ear” for what should sound like similar output. \n",
    "As stated above, although the majority of literature in AI music has focused on generative models to compose music in a particular style or genre, none has focused in particular on the task of completing a specific piece of music. In terms of the overall model and training architectures, there is not necessarily any novelty compared to the literature, however the problem formulation itself is unique. This unique approach lends itself to calculating various loss and accuracy scores since we know exactly what the results should be (for the other half). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSk8Cb1US_8F"
   },
   "source": [
    "#### **Data**\n",
    "\n",
    "1. Dataset selection and File Format Conversion\n",
    "\n",
    "Upon researching different types of datasets, we have decided to select midi files as our source as the format conserves piano tunes in a finer form compared to other instrumentals or audio files, which may add different types of noise into our data. And we decided to choose a dataset from a reliable source - Kaggle, and found a dataset called ‘Classical Music MIDI’ with a total 7 MB of data with over thousands of downloads history ( https://www.kaggle.com/soumikrakshit/classical-music-midi ). Considering we had to train the models with songs from one composer, we selected a composer with the most number of songs, which was Chopin. We initially started with the 48 midi files of Chopin’s different piano pieces, however converted them to wav files in order to pre-process them into spectogram arrays and segmented the wav files into 15 seconds time intervals with 5 seconds of overlap, resulting in over 2700 wav files.\n",
    "\n",
    "2. Dividing the data into Train-Test, Input-Output\n",
    "\n",
    "We splitted the total dataset into train, test and input, output data before the conversion of wav files into arrays since splitting them after converting them to arrays resulted in loss in sound quality. After we have splitted the data into four parts, with the train and test ratio as 5:1, we converted the wav files into spectogram arrays using Fluidsynth library. Since the spectogram in general shows the power of various frequencies in the song over time in an image, it allowed us to train the dataset with machine learning models such as CNN, Autoencoder, DCGAN, and PIX2PIX.\n",
    "\n",
    "3. Cleaning and Pre-processing the Data\n",
    "\n",
    "Before converting the wav files into spectograms, any files that were cut off under 15 seconds - the leftover segments were removed to reduce any outliers. And using Librosa, the waveplots and spectograms were visualized to make sure the spectograms were converted adequately. The segmenting process used AudioSegment function from Pydub library used to manipulate audio with high level interface in python, preserving the audio quality while dividing them into smaller pieces. In addition, with the Librosa library, the wav files were converted into a spectogram with the method of squaring the magnitude of the short term Fourier transform (STFT), then compressed to a mel-spectograms. This made sure all the data input and outputs were in range of appropriate spectogram scale. Each spectogram arrays were stacked together to  In addition, Bethold converted the spectograms back to wav files to ensure the rhythm and tempo is preserved. Below shows the original audio and that of the spectrogram.\n",
    "\n",
    "\n",
    "Figure 1. Image show comparison of original audio file and audio from spectrogram, which look about the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Reshaping\n",
    "\n",
    "Based on different models, the spectrogram arrays had to be reshaped in order to fit the models. For the baseline models, such as Random Forest and Linear Regression, the input and output shapes were flattened to 1D arrays with reshape(-1,1) function in order to be trained to evaluate mae and mse scores. And as the conversion between melspectogram and wav file required a 2D array, the dimension of the stacked 3D model was expanded to 4D array for regression prediction with np.expand_dims(data, -1). The final shape of train and test data for CNN models were (1275, 128, 646, 1) and (158, 128, 646, 1).\n",
    "\n",
    "For Autoencoder, dimensions for the input were (1575, 256, 64, 1) and the same for the decoder, while the dimension of the latent space was 2. And for Transformer, GANs, and Pix2Pix models, an additional procedure of converting the grayscale spectogram to color spectogram was used to conserve more data, resulting in train and test data shapes of (2, 256, 300, 128, 1) and (2, 20, 300, 128, 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThlBUkGlMmlh"
   },
   "source": [
    "#### **Methods**\n",
    "\n",
    "\n",
    "1. Baseline Models: Linear Regression and Random Forest\n",
    "\n",
    "For the baseline models, the input and output shapes were flattened to 1D arrays with reshape(-1,1) function in order to be trained to evaluate mae and mse scores. Simple logistic regression was performed without any regularization and following is the result, the mae score of 9.34 and mse score of 167.16.\n",
    "\n",
    "<img src=\"results/lg_result.PNG\" width=\"400\"> \n",
    "\n",
    "Meanwhile the Random Forest model was set up with the optimal architecture after cross-validation as the following :\n",
    "\n",
    "<img src=\"results/rf_model.PNG\" width=\"700\"> \n",
    "\n",
    "After training with the normalized train dataset and its target, the model was evaluated on the test dataset and as a result, an R square of 0.4 was obtained. Another metric of R square was utilized here as the baseline model is performing a regression prediction based on the train and its target dataset.\n",
    "\n",
    "\n",
    "\n",
    "2. CNN : Hannah\n",
    "\n",
    "From the prepared spectogram arrays of 1275 train data and 158 test data for each input and output set, three different types of CNN models were run to predict the output array. The first model was a basic CNN model with minimum parameters with max pooling, drop out, conv 2d, transpose, and leaky relu layers embedded one time each. Total and trainable number of parameters were 29 and the model was compiled with adam optimizer to predict the mse and mae scores. \n",
    "\n",
    "  <img src=\"results/cnn_model1.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model1_result.PNG\" width=\"400\"> \n",
    "\n",
    "\n",
    "Running 10 epochs of model one definitely showed steady decrease in loss value, however did not converge fast enough and could be run with more epochs for better fitting.\n",
    "\n",
    "\n",
    "The second model was developed with more deeper convolution layers, with narrower layers with only few nodes in each layer, a total number of parameters of 800.\n",
    "<img src=\"results/cnn_model2.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model2_result.PNG\" width=\"400\"> \n",
    "The training and validation loss overlapped around epoch eight for model two. The training loss may continue to steadily decrease, but the model training seems to be stopped early for generalization trade-off. The loss scores were much lower than the first model.\n",
    "\n",
    "\n",
    "The third model was developed with more deep convolution layers, along with additional nodes on each layers, the total number of parameters of 38,611. \n",
    "<img src=\"results/cnn_model3.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model3_result.PNG\" width=\"400\"> \n",
    "The validation loss started to increase around epoch seven for model three. And following is a graph summarizing performance of all three models.\n",
    "\n",
    "<img src=\"results/cnn_models_result.PNG\" width=\"400\"> \n",
    "Here, we can see that model one is out of bounds since the convergence is too slow and model three definitely performs the best. Observing this result, I chose model three to train my dataset and the detailed evaluation will be continued on the next section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "AutoEncoder : Ethan\n",
    "\n",
    "Borrowing from Valerio Velardo, a notable figure in the AI Audio community, I implemented a preprocessing pipeline that involves the following steps: \n",
    "Load the midi files\n",
    "Convert the midi files to wav files\n",
    "Left or right pad the signal (if necessary)\n",
    "Extract log spectrogram of signal\n",
    "Save normalized spectrogram as .npy file\n",
    "At the end of this preprocessing pipeline, you end up with a directory that contains all of the normalized log spectrograms of the samples, as well as a pickle file file that contains the min and max values of the spectrograms, to be used for normalization (before being fed into the model) and eventually for denormalization (to go back to normal audio after conversion from spectrogram to audio). Log spectrograms were extracted using librosa’s short-time Fourier transform function with log scaling. \n",
    "\n",
    "As for the Variational Autoencoder, I used a 5 convolutional layer architecture with mirrored encoder and decoder components and a two-dimensional latent space. I trained it for 100 epochs across all 1575 samples (extracted from original full MIDI files of Chopin pieces). \n",
    "\n",
    "\n",
    "\n",
    "Transformer, GANs, Pix2Pix : Bethold\n",
    "\n",
    "I converted each of the spectrograms to a numpy array of (512,128), then I splitted into input data and target data of (256, 128) array respectively. Thus given an input audio, the goal is to predict the rest of the audio song. So the 15 seconds audio files were split into 5 seconds input and 5 seconds target. Below shows sample spectrogram for input and target 5 seconds audio files.\n",
    "\n",
    "<img src=\"results/inp_target.JPG\" width=\"400\"> \n",
    "Figure 2: Above shows sample spectrograms of input and target of 5 seconds respectively\n",
    "\n",
    "Total input and target data were (1431, 256, 128) numpy arrays respectively. From here I normalized the data to be between 0 and 1 by subtracting each value from the max and dividing the entire values by max. In this work there was no need to explore univariate and multivariate feature selection or dimension reduction  because the wav files transformed into mel-spectrograms had the important features preserved. \n",
    "\n",
    "Next, I split the data into training and test data. The train consisted of 0.9 and test data consisted of 0.1 of the total input and target data. This gave us a training dataset of (1287, 256, 128) and test set of (144, 256, 128). \n",
    "\n",
    "Base models I utilized are Random Forest and tenor Regressor. I implemented 4 different models in addition to the base models. These models include Auto Encoders, Variational AutoEncoders, Deep Convolutional Generative Adversarial Network (DCGAN) and PIX2PIX. I utilized these models because we were dealing with image spectrograms and predicting pixel values. \n",
    "\n",
    "Hyperparameters I utilized include the number of hidden layers, filters and activation functions in each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NG1mWiZMmSo"
   },
   "source": [
    "#### **Evaluation**\n",
    "\n",
    "1. CNN : Hannah\n",
    "\n",
    "\n",
    "Based on the previous results, model three was trained with the optimal epochs of seven and following is the final graph of training and validation loss for model three. Final mae score resulted in 8.9 for training and 8.4 for test data, much better than the baseline models.\n",
    "<img src=\"results/cnn_model3b_result.PNG\" width=\"400\"> \n",
    "<img src=\"results/cnn_model3c_result.PNG\" width=\"400\"> \n",
    "\n",
    "\n",
    "2. AutoEncoder : Ethan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Transformer, GANs, Pix2Pix : Bethold\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azibEYIUMm6Q"
   },
   "source": [
    "#### **Conclusion**\n",
    "How well did it work? Characterize how robust you think the results are (did you have enough data?) Try for interpretation of what the model found (what variables were useful, what was not)? Try to avoid describing what you would do if you had more time. If you have to make a statement about “future work” limit it to one short statement.\n",
    "\n",
    "\n",
    "\n",
    "**V. Visualization 10pts**\n",
    "\n",
    "A. Do you provide visualization summaries for all your data and features?\n",
    "\n",
    "B. Do you use the correct visualization type, eg. bar graphs for categorical data, scatter plots for numerical data, etc?\n",
    "\n",
    "C. Are your axes properly labeled?\n",
    "\n",
    "D. Do you use color properly?\n",
    "\n",
    "E. Do you use opacity and dot size so that scatterplots with lots of data points are not just a mass of interpretable dots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suxnAKw8TAPX"
   },
   "source": [
    "#### **Attribution**\n",
    "\n",
    "Using the number and size of github commits by author (bar graph), and the git hub visualizations of when the commits occurred. Using these measures each person should self-report how many code-hours of their work are visible in the repo with 2-3 sentences listing their contribution. Do not report any code hours that cannot be traced to commits. If you spend hours on a 2-line change of code or side-reading you did, you cannot report. If you do searches or research for the project that does not result in code, you must create notes in a markdown file (eg. in the project wiki) and the notes should be commensurate with the amount of work reported. Notes cannot be simply copy-pasted from elsewhere (obviously).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sqxuihTUP9f"
   },
   "source": [
    "#### **Bibliography**\n",
    "\n",
    "References should appear at the end of the report/notebook. Again, no specific format is required but be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bvcG3f0UZzm"
   },
   "source": [
    "#### **Appendix**\n",
    "\n",
    "If there are minor results and graphs that you think should be included, put them at the end. Do not include anything without an explanation. No random graphs just for padding!! However, let’s say you did a 50 state analysis of poverty and demographics, and your report focused on the 5 most interesting states, for completeness you could include all in an appendix. Be sure though to provide some (very short) discussion with each figure/code/result.\n",
    "\n",
    "Again, this is not an opportunity to explain what you learned on the project or in the course, or express regret on mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udoNHPbHWgWW"
   },
   "source": [
    "**Criteria for Code and Presentation** (Omit this cell after completing code and presentation - this does NOT go into presentation)\n",
    "\n",
    "VI. Code 20pts\n",
    "\n",
    "A. Is the code provided can reproduce the entire work?\n",
    "\n",
    "B. Is the data included or at least linked (externally) with instructions on how to download it?\n",
    "\n",
    "C. Do you factor repeated operations into functions to avoid repetitively and error-prone copy-paste?\n",
    "\n",
    "E. Do you use docstrings and numpy documentation style:\n",
    "   https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt\n",
    "   to make your code clear and readable?\n",
    "\n",
    "F. Do you use markdown cells to explain every step of your code similar to Homeworks and some example notebooks?\n",
    "\n",
    "G. Does the code demonstrate considerable work given the number of people on the project?\n",
    "VII. Presentation 30pts\n",
    "\n",
    "A. Do you tell a coherent story with a beginning, middle, and end?\n",
    "\n",
    "B. Do you introduce why the problem is important?\n",
    "\n",
    "C. Do you explain in the first couple of slides what you accomplished on solving the problem?\n",
    "\n",
    "D. Are you careful not to have slides filled with text (keep in notes)?\n",
    "\n",
    "E. Is data and evaluations presented as clear figures (mostly)?\n",
    "\n",
    "F. Do you make sure to say what is \"interesting\" or should be learned from each figure?\n",
    "\n",
    "G. Do you stay within your time limits 15 min?\n",
    "\n",
    "H. Do you avoid useless padding slides of no relevance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNDyjNe6Mlun"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcQeqoepU0/qpSSV8Z+HmX",
   "include_colab_link": true,
   "name": "Report_RecomposeClassics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CSCCNY/final-project-recomposeclassics/blob/main/Report_RecomposeClassics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGogWFK7MpaQ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **Compose Chopin**\n",
    "#### Learn Chopin's style of composition through CNN, LSTM, AutoEncoder, and GANs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef9ScunNMpI4"
   },
   "source": [
    "Bethold Owusu, Hannah Do, Ethan Cobb\n",
    "\n",
    "**CSCI1910**  /  Spring 2021\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ztB3WbIMo0Y"
   },
   "source": [
    "#### **Abstract**\n",
    "\n",
    "Our project aims to learn Chopin's style of composition through various DNN models such as CNN, LSTM, AutoEncoder, and GANs. Training a machine how to compose has been in demand for awhile, and many state of the art models such as Amper Music, AIVA, Jukedeck, and others have been developed with AI composers. However, no commercial AI system has been good enough by itself, and many composers use it as a mere reference. Therefore, we have taken a different approach in converting the wav files into spectrograms, and predicting the pieces as an image analysis process. In other words, our project focuses on predicting the next sequence of a song given a previous sequence of a song using spectrogram pixel values. We have developed different DNN models to produce machine-composed segments that resemble target music pieces with high similarity scores. Converting the midi and wav files into spectrograms unavoidably added some noises to the song, however complex CNN model with transpose layers and AutoEncoder model resulted in higher prediction accuracy than the Base models with low mse and mae scores when compared with the actual sequence. In addition, VAE, Transformer, DCGAN, and PIX2PIX models were tested and the last model, PIX2PIX resulted in highly accurate conversion in recreating the sequence of the songs. Having such a result, we hope to expand this project not only in Chopin style, but to different composing styles in order to allow composing songs as easy as clicking a button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9MvKNBXS8xX"
   },
   "source": [
    "Our research question was how to train the song sequences from a dataset to predict a new sequence, enabling a trained machine to compose songs by itself. To begin with, we selected a classical music midi file dataset from Kaggle. Compared to other datasets that contain different instrumentals and information about the file itself, our goal was to train the model with a clean midi file dataset that only contains piano tunes in refined forms. As we wanted to predict a composition style of a single composer, the composer with the most songs in the dataset - Chopin was selected.\n",
    "In researching different types of ways to pre-process the midi files, we found many state of the art models using spectrogram conversion for data analysis, using image analysis method to extract a new spectrogram to predict following sequences. As a spectrogram shows the power of various frequencies in the song over time in an image, we decided to move on with converting the midi files to spectrograms. The specific process included processing the midi into wav files first, then converting the wav files into spectrograms through Librosa library. And with the spectrogram arrays stacked together to form 3D numpy arrays, different DNN models such as CNN, LSTM, Transformer, AutoEncoders, DCGAN, Pix2Pix were trained to predict the final sequence array. The procedure and detailed results are explained in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFdPciEPS_qu"
   },
   "source": [
    "#### **Background**\n",
    "Discuss other relevant work on solving this problem. Most of your references are here. Cite all sources. There is no specific formatting requirement for citations but be consistent.\n",
    "\n",
    "**I Criteria - Machine Learning Question: 20 pts**\n",
    "\n",
    "A. Is the background context for the question stated clearly (with references)?\n",
    "\n",
    "B. Is the hypothesis/problem stated clearly (\"The What\")\n",
    "\n",
    "C. Is it clear why the problems are important? Is it clear why anyone would care? (\"The Why\")\n",
    "\n",
    "D. Is it clear why the data were chosen should be able to answer the question being asked?\n",
    "\n",
    "E. How new, non-obvious, significant are your problems? Do you go beyond checking the easy and obvious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSk8Cb1US_8F"
   },
   "source": [
    "#### **Data**\n",
    "\n",
    "1. Dataset selection and File Format Conversion\n",
    "\n",
    "Upon researching different types of datasets, we have decided to select midi files as our source as the format conserves piano tunes in a finer form compared to other instrumentals or audio files, which may add different types of noise into our data. And we decided to choose a dataset from a reliable source - Kaggle, and found a dataset called ‘Classical Music MIDI’ with a total 7 MB of data with over thousands of downloads history ( https://www.kaggle.com/soumikrakshit/classical-music-midi ). Considering we had to train the models with songs from one composer, we selected a composer with the most number of songs, which was Chopin. We initially started with the 48 midi files of Chopin’s different piano pieces, however converted them to wav files in order to pre-process them into spectogram arrays and segmented the wav files into 15 seconds time intervals with 5 seconds of overlap, resulting in over 2700 wav files.\n",
    "\n",
    "2. Dividing the data into Train-Test, Input-Output\n",
    "\n",
    "We splitted the total dataset into train, test and input, output data before the conversion of wav files into arrays since splitting them after converting them to arrays resulted in loss in sound quality. After we have splitted the data into four parts, with the train and test ratio as 5:1, we converted the wav files into spectogram arrays using Fluidsynth library. Since the spectogram in general shows the power of various frequencies in the song over time in an image, it allowed us to train the dataset with machine learning models such as CNN, Autoencoder, DCGAN, and PIX2PIX.\n",
    "\n",
    "3. Cleaning and Pre-processing the Data\n",
    "\n",
    "Before converting the wav files into spectograms, any files that were cut off under 15 seconds - the leftover segments were removed to reduce any outliers. And using Librosa, the waveplots and spectograms were visualized to make sure the spectograms were converted adequately. The segmenting process used AudioSegment function from Pydub library used to manipulate audio with high level interface in python, preserving the audio quality while dividing them into smaller pieces. In addition, with the Librosa library, the wav files were converted into a spectogram with the method of squaring the magnitude of the short term Fourier transform (STFT), then compressed to a mel-spectograms. This made sure all the data input and outputs were in range of appropriate spectogram scale. Each spectogram arrays were stacked together to  In addition, Bethold converted the spectograms back to wav files to ensure the rhythm and tempo is preserved. Below shows the original audio and that of the spectrogram.\n",
    "\n",
    "\n",
    "Figure 1. Image show comparison of original audio file and audio from spectrogram, which look about the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from results folder : spectogram to audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Reshaping\n",
    "\n",
    "Based on different models, the spectogram arrays had to be reshaped in order to fit the models. For the baseline models, such as Random Forest and Linear Regression, the input and output shapes were flattened to 1D arrays with reshape(-1,1) function in order to be trained to evaluate mae and mse scores. And as the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThlBUkGlMmlh"
   },
   "source": [
    "#### **Methods**\n",
    "How did you take your data and set up the problem? Describe things like normalization, feature selection, the models you chose. In this section, you may have EDA and graphs showing the exploration of hyper-parameters. Note: Use graphs to illustrate interesting relationships that are important to your final analyses. DO NOT just show a bunch of graphs because you can. You should label and discuss every graph you include. There is no required number to include. The graphs should help us understand your analysis process and illuminate key features of the data.\n",
    "\n",
    "**III. Criteria - Transformation, Feature Selection, and Modeling: 30pts**\n",
    "\n",
    "\n",
    "\n",
    "A. Did you transform, normalize, filter the data appropriately to solve your problem? Did you divide by max-min, or the sum, root-square-sum, or did you z-score the data? Did you justify what you did?\n",
    "B. Did you justify normalization or lack of checking which works better as part of your hyper-parameters?\n",
    "C. Did you explore univariate and multivariate feature selection? (if not why not)\n",
    "D. Did you try dimension reduction and which methods did you try? (if not why not)\n",
    "E. Did you include 1-2 simple models, for example with classification LDA, Logistic Regression or KNN?\n",
    "F. Did you pick an appropriate set of models to solve the problem? Did you justify why these models and not others?\n",
    "G. Did you try at least 4 models including one Neural Network Model using Tensor-Flow or Pytorch?\n",
    "H. Did you exercise the data science models/problems we described in the lectures showing what was presented?\n",
    "I. Are you using appropriate hyper-parameters? For example, if you are using a KNN regression are you investigating the choice of K and whether you use uniform or distance weighting? If you are using K-means do you explain why K? If you are using PCA do you explore how many dimensions such as by looking at the eigenvalues?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NG1mWiZMmSo"
   },
   "source": [
    "#### **Evaluation**\n",
    "Here you are going to show your different models’ performance. It is particularly useful to show multiple metrics and things like ROC curves (for binary classifiers). Make sure it is clearly not just what the score is but for which instances in the data one has the largest errors (in a regression), or just sample examples miss-classified. Make an attempt to interpret the parameters of the model to understand what was useful about the input data. Method comparison and sensitivity analyses are absolutely CRUCIAL to good scientific work. To that end, you MUST compare at least 2 different methods from class in answering your scientific questions. It is important to report what you tried but do so SUCCINCTLY.\n",
    "\n",
    "\n",
    "\n",
    "**IV. Criteria : Metrics, Validation and Evaluation 20pts**\n",
    "\n",
    "A. Are you using an appropriate choice of metrics? Are they well justified? If you are doing classification do you show a ROC curve? If you are doing regression are you justifying the metric least squares vs. mean absolute error? Do you show both?\n",
    "\n",
    "B. Do you validate your choices of hyperparameters? For example, if you use KNN or K-means do you use cross-validation to optimize your choice of parameters?\n",
    "\n",
    "C. Did you make sure your training and validation process never used the training data?\n",
    "\n",
    "D. Do you estimate the uncertainty in your estimates using cross-validation?  \n",
    "E. Can you say how much you are overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azibEYIUMm6Q"
   },
   "source": [
    "#### **Conclusion**\n",
    "How well did it work? Characterize how robust you think the results are (did you have enough data?) Try for interpretation of what the model found (what variables were useful, what was not)? Try to avoid describing what you would do if you had more time. If you have to make a statement about “future work” limit it to one short statement.\n",
    "\n",
    "\n",
    "\n",
    "**V. Visualization 10pts**\n",
    "\n",
    "A. Do you provide visualization summaries for all your data and features?\n",
    "\n",
    "B. Do you use the correct visualization type, eg. bar graphs for categorical data, scatter plots for numerical data, etc?\n",
    "\n",
    "C. Are your axes properly labeled?\n",
    "\n",
    "D. Do you use color properly?\n",
    "\n",
    "E. Do you use opacity and dot size so that scatterplots with lots of data points are not just a mass of interpretable dots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suxnAKw8TAPX"
   },
   "source": [
    "#### **Attribution**\n",
    "\n",
    "Using the number and size of github commits by author (bar graph), and the git hub visualizations of when the commits occurred. Using these measures each person should self-report how many code-hours of their work are visible in the repo with 2-3 sentences listing their contribution. Do not report any code hours that cannot be traced to commits. If you spend hours on a 2-line change of code or side-reading you did, you cannot report. If you do searches or research for the project that does not result in code, you must create notes in a markdown file (eg. in the project wiki) and the notes should be commensurate with the amount of work reported. Notes cannot be simply copy-pasted from elsewhere (obviously).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sqxuihTUP9f"
   },
   "source": [
    "#### **Bibliography**\n",
    "\n",
    "References should appear at the end of the report/notebook. Again, no specific format is required but be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bvcG3f0UZzm"
   },
   "source": [
    "#### **Appendex**\n",
    "\n",
    "If there are minor results and graphs that you think should be included, put them at the end. Do not include anything without an explanation. No random graphs just for padding!! However, let’s say you did a 50 state analysis of poverty and demographics, and your report focused on the 5 most interesting states, for completeness you could include all in an appendix. Be sure though to provide some (very short) discussion with each figure/code/result.\n",
    "\n",
    "Again, this is not an opportunity to explain what you learned on the project or in the course, or express regret on mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udoNHPbHWgWW"
   },
   "source": [
    "**Criteria for Code and Presentation** (Omit this cell after completing code and presentation - this does NOT go into presentation)\n",
    "\n",
    "VI. Code 20pts\n",
    "\n",
    "A. Is the code provided can reproduce the entire work?\n",
    "\n",
    "B. Is the data included or at least linked (externally) with instructions on how to download it?\n",
    "\n",
    "C. Do you factor repeated operations into functions to avoid repetitively and error-prone copy-paste?\n",
    "\n",
    "E. Do you use docstrings and numpy documentation style:\n",
    "   https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt\n",
    "   to make your code clear and readable?\n",
    "\n",
    "F. Do you use markdown cells to explain every step of your code similar to Homeworks and some example notebooks?\n",
    "\n",
    "G. Does the code demonstrate considerable work given the number of people on the project?\n",
    "VII. Presentation 30pts\n",
    "\n",
    "A. Do you tell a coherent story with a beginning, middle, and end?\n",
    "\n",
    "B. Do you introduce why the problem is important?\n",
    "\n",
    "C. Do you explain in the first couple of slides what you accomplished on solving the problem?\n",
    "\n",
    "D. Are you careful not to have slides filled with text (keep in notes)?\n",
    "\n",
    "E. Is data and evaluations presented as clear figures (mostly)?\n",
    "\n",
    "F. Do you make sure to say what is \"interesting\" or should be learned from each figure?\n",
    "\n",
    "G. Do you stay within your time limits 15 min?\n",
    "\n",
    "H. Do you avoid useless padding slides of no relevance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNDyjNe6Mlun"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcQeqoepU0/qpSSV8Z+HmX",
   "include_colab_link": true,
   "name": "Report_RecomposeClassics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
